{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788a9b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Getting Data---\n",
      "Loaded: Streaming_History_Audio_2021_8.json\n",
      "Loaded: Streaming_History_Audio_2022_12.json\n",
      "Loaded: Streaming_History_Audio_2021-2022_10.json\n",
      "Loaded: Streaming_History_Audio_2023-2024_17.json\n",
      "Loaded: Streaming_History_Audio_2019_1.json\n",
      "Loaded: Streaming_History_Audio_2020_4.json\n",
      "Loaded: Streaming_History_Audio_2020_3.json\n",
      "Loaded: Streaming_History_Audio_2021_9.json\n",
      "Loaded: Streaming_History_Audio_2021_6.json\n",
      "Loaded: Streaming_History_Audio_2025_22.json\n",
      "Loaded: Streaming_History_Audio_2024_18.json\n",
      "Loaded: Streaming_History_Audio_2021_7.json\n",
      "Loaded: Streaming_History_Audio_2020-2021_5.json\n",
      "Loaded: Streaming_History_Audio_2024-2025_20.json\n",
      "Loaded: Streaming_History_Audio_2024_19.json\n",
      "Loaded: Streaming_History_Audio_2022_11.json\n",
      "Loaded: Streaming_History_Audio_2023_14.json\n",
      "Loaded: Streaming_History_Audio_2023_16.json\n",
      "Loaded: Streaming_History_Audio_2019-2020_2.json\n",
      "Loaded: Streaming_History_Audio_2023_15.json\n",
      "Loaded: Streaming_History_Audio_2015-2019_0.json\n",
      "Loaded: Streaming_History_Audio_2025_21.json\n",
      "Loaded: Streaming_History_Audio_2022-2023_13.json\n",
      "\n",
      "Total raw streams loaded: 359043\n",
      "âœ… Sensitive columns scrubbed from data in memory.\n",
      "Total streams after removing skips (<30s): 124280\n",
      "saved clean history!\n",
      "Saved 8520 unique tracks\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "project_path = '/home/me/dev/spotanalysis'\n",
    "\n",
    "\n",
    "raw_data = os.path.join(project_path, 'data', 'raw')\n",
    "processed_data = os.path.join(project_path,'data', 'processed')\n",
    "all_streams = []\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "print(\"---Getting Data---\")\n",
    "for filename in os.listdir(raw_data):\n",
    "    if filename.startswith('Streaming_History_Audio') and filename.endswith('.json'):\n",
    "        file_path = os.path.join(raw_data, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                all_streams.extend(data)\n",
    "                print(f\"Loaded: {filename}\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error reading JSON from {filename}. Skipping.\")\n",
    "\n",
    "# Convert dictionaries to pandas Data Frame\n",
    "df_history = pd.DataFrame(all_streams)\n",
    "print(f\"\\nTotal raw streams loaded: {len(df_history)}\")\n",
    "\n",
    "drop_columns = ['ip_addr', 'conn_country', 'platform', 'incognito_mode', ' offline_timestamp']\n",
    "df_history.drop(columns=drop_columns, inplace=True, errors='ignore')\n",
    "print(\"Sensitive columns scrubbed from data in memory.\")\n",
    "\n",
    "#convert endtime column to datetime endtime\n",
    "df_history['ts'] = pd.to_datetime(df_history['ts'])\n",
    "\n",
    "#Songs under 30s don't count as a stream. \n",
    "df_history_clean = df_history[df_history['ms_played'] >= 30000].copy()\n",
    "print(f\"Total streams after removing skips (<30s): {len(df_history_clean)}\")\n",
    "\n",
    "#cleaning \n",
    "df_history_clean['HourOfDay'] = df_history_clean['ts'].dt.hour\n",
    "df_history_clean['DayOfWeek'] = df_history_clean['ts'].dt.day_name()\n",
    "df_history_clean['Date'] = df_history_clean['ts'].dt.date\n",
    "\n",
    "#Removing repeat songs\n",
    "unique_tracks = df_history_clean[['master_metadata_album_artist_name', 'master_metadata_track_name']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "df_history_clean.to_csv(os.path.join(processed_data,'clean_history.csv'), index=False)\n",
    "unique_tracks.to_csv(os.path.join(processed_data, 'unique_tracks.csv'), index=False)\n",
    "\n",
    "print(\"saved clean history!\")\n",
    "print(f\"Saved {len(unique_tracks)} unique tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55a68f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded local tracks: 8520 rows. (Optimized)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11289/769254871.py:60: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_external = pd.read_csv(EXTERNAL_FEATURES_PATH, usecols=external_cols_to_use)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded external features: 42305 rows. (Optimized)\n",
      "\n",
      "[CLEANUP] Freed up memory from large input DataFrames.\n",
      "\n",
      "==================================================\n",
      "DATA MERGE COMPLETE\n",
      "Total Unique Tracks: 8520\n",
      "Tracks enriched with features: 2379 (27.92%)\n",
      "Final data shape: (8520, 13)\n",
      "==================================================\n",
      "Final data saved to: /home/me/dev/spotanalysis/data/processed/enriched_tracks.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Spotify removed their API so external audio feature file was used from Kaggle\n",
    "# External data does not have artist column so just merged with track names and dropped the dupes. S\n",
    "\n",
    "\n",
    "CURRENT_DIR = os.getcwd() \n",
    "\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(CURRENT_DIR) \n",
    "\n",
    "# Define paths relative to the project root\n",
    "PROCESSED_FOLDER = os.path.join(PROJECT_ROOT, 'data', 'processed')\n",
    "RAW_FOLDER = os.path.join(PROJECT_ROOT, 'data', 'raw')\n",
    "\n",
    "# Input File Paths\n",
    "LOCAL_TRACKS_PATH = os.path.join(PROCESSED_FOLDER, 'unique_tracks.csv')\n",
    "EXTERNAL_FEATURES_PATH = os.path.join(RAW_FOLDER, 'spot_features.csv') \n",
    "\n",
    "# Output File Path\n",
    "OUTPUT_PATH = os.path.join(PROCESSED_FOLDER, 'enriched_tracks.csv')\n",
    "\n",
    "\n",
    "\n",
    "# List of all the feature columns\n",
    "FEATURE_COLS = [\n",
    "    'danceability', 'energy', 'valence', 'loudness', 'mode', 'speechiness', \n",
    "    'acousticness', 'instrumentalness', 'liveness', 'tempo', 'key'\n",
    "]\n",
    "\n",
    "# Column Names for Local Files\n",
    "LOCAL_ARTIST_COL = 'master_metadata_album_artist_name' \n",
    "LOCAL_TRACK_COL = 'master_metadata_track_name' \n",
    "\n",
    "# EXTERNAL FEATURE FILE (40k songs)\n",
    "EXTERNAL_ARTIST_COL = 'genre' \n",
    "EXTERNAL_TRACK_COL = 'song_name' \n",
    "\n",
    "\n",
    "# Load Dataframes\n",
    "\n",
    "try:\n",
    "    \n",
    "    local_cols_to_use = [LOCAL_ARTIST_COL, LOCAL_TRACK_COL]\n",
    "    \n",
    "    df_local = pd.read_csv(LOCAL_TRACKS_PATH, usecols=local_cols_to_use)\n",
    "    print(f\"Loaded local tracks: {len(df_local)} rows. (Optimized)\")\n",
    "    \n",
    "    \n",
    "    external_cols_to_use = [EXTERNAL_ARTIST_COL, EXTERNAL_TRACK_COL] + FEATURE_COLS\n",
    "    \n",
    "    \n",
    "    df_external = pd.read_csv(EXTERNAL_FEATURES_PATH, usecols=external_cols_to_use)\n",
    "    print(f\"Loaded external features: {len(df_external)} rows. (Optimized)\")\n",
    "\n",
    "# error diagnosis\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: Could not find a file. Please ensure the path is correct: {e}\")\n",
    "    print(f\"Checked in project root: {PROJECT_ROOT}\")\n",
    "    \n",
    "\n",
    "    try:\n",
    "        print(\"\\n--- DIAGNOSIS: Listing contents of the 'data/raw' folder ---\")\n",
    "        contents = os.listdir(RAW_FOLDER)\n",
    "        print(\"Found files:\")\n",
    "        for item in contents:\n",
    "            print(f\"- {item}\")\n",
    "        print(\"\\nACTION: Please check the list above for the correct feature file name (it might have different capitalization or spelling).\")\n",
    "        print(\"Then, update the variable 'EXTERNAL_FEATURES_PATH' in Step 1 and re-run.\")\n",
    "    except Exception as list_e:\n",
    "        print(f\"Could not list directory contents: {list_e}\")\n",
    "    \n",
    "    \n",
    "    sys.exit()\n",
    "except ValueError as e:\n",
    "    # Catches the ValueError related to missing columns in usecols \n",
    "    print(f\"ERROR: Column loading failed (ValueError): {e}\")\n",
    "    print(\"\\nFATAL ERROR: Failed to load external features. This should have been fixed by the previous step.\")\n",
    "    sys.exit()\n",
    "\n",
    "except KeyError as e:\n",
    "    # Catches KeyErrors if they occur later during processing\n",
    "    print(f\"ERROR: One of your configured column names is wrong: {e}\")\n",
    "    print(\"Please go back and update the variables in '--- 3. Configuration for Merge Keys ---'\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# Prep Merge Keys\n",
    "\n",
    "# Lowercase local merge key\n",
    "df_local['merge_key'] = (df_local[LOCAL_TRACK_COL].astype(str)).str.lower().str.strip()\n",
    "\n",
    "# Lowercase external merge key\n",
    "df_external['merge_key'] = (df_external[EXTERNAL_TRACK_COL].astype(str)).str.lower().str.strip()\n",
    "\n",
    "# No dupes\n",
    "df_external_features = df_external[['merge_key'] + FEATURE_COLS].drop_duplicates(subset=['merge_key'], keep='first')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use a LEFT merge to keep ALL of the local tracks\n",
    "df_enriched = pd.merge(\n",
    "    df_local,\n",
    "    df_external_features,\n",
    "    on='merge_key',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "# Delete the large DataFrames that are no longer needed\n",
    "del df_external\n",
    "del df_local\n",
    "del df_external_features # Delete temporary feature DF\n",
    "print(\"\\n[CLEANUP] Freed up memory from large input DataFrames.\")\n",
    "\n",
    "\n",
    "\n",
    "# Clean up the temporary merge key column\n",
    "df_enriched = df_enriched.drop(columns=['merge_key'])\n",
    "\n",
    "# Check and report success rate\n",
    "matched_tracks = df_enriched['energy'].notna().sum()\n",
    "total_tracks = len(df_enriched)\n",
    "match_rate = (matched_tracks / total_tracks) * 100 if total_tracks > 0 else 0\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"DATA MERGE COMPLETE\")\n",
    "print(f\"Total Unique Tracks: {total_tracks}\")\n",
    "print(f\"Tracks enriched with features: {matched_tracks} ({match_rate:.2f}%)\")\n",
    "print(f\"Final data shape: {df_enriched.shape}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save the final enriched DataFrame to the processed folder\n",
    "df_enriched.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Final data saved to: {OUTPUT_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spotanalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
